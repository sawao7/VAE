# **VAE(変分オートエンコーダー)をpytorchを用いて実装**
### **①オートエンコーダーとは**
変分オートエンコーダーを説明する前に、まずはオートエンコーダーについて解説する。
オートエンコーダーの特徴は、
1.   教師データが存在しないこと
2.   入力データと同じデータを返すように学習する
3.   入力データの重要な特徴量だけを残すことで次元削減が可能

この２点である。
例えば、今回使用するMNISTの画像を入力すると、同じ画像が出力される。

### **②VAE(変分オートエンコーダー)とは**
オートエンコーダーのエンコーダーとデコーダーの間に存在する潜在変数zに何かしらの確率分布が存在すると過程して学習する。
※以下数式を用いた説明
今回は、エンコーダーにガウス分布、デコーダーにベルヌーイ分布を用いる。


*   エンコーダー(ガウス分布) $$q_{\phi}({\bf z}|{\bf x}) = {\mathcal N}({\bf z}| g^{\mu}_{\phi}({\bf x}),(g^{\sigma}_{\phi}({\bf x})^2{\bf I}) $$
*   デコーダ（ベルヌーイ分布）：$$p_{\theta}({\bf x}|{\bf z}) = Ber({\bf x}| f_{\theta}({\bf z}))$$

次に、VAEの目的関数。
$$
 {\mathcal L}({\bf x};{\bf \theta},{\bf \phi}) = \mathbb{E}_{q_{\phi}({\bf z}|{\bf x})}[\log p_\theta({\bf x}|{\bf z})] -D_{KL}[q_{\phi}({\bf z}|{\bf x})||p_{\theta}({\bf z})]
 $$  

 第一項が再構成したときに生じる誤差、第二項は正則化の項
KLとは、KLダイバージェンスのことで、必ず非負の値を取るため、ここでは細かい変換は割愛するが、結局は対数周辺尤度$$log p_\theta(x)$$を最大化することは目的関数を最大化することと同じになる。

目的関数の第一項は, 次のようにサンプル近似できる.
$$
  \mathbb{E}_{q_{\phi}({\bf z}|{\bf x})}[\log p_\theta({\bf x}|{\bf z})]\simeq \frac{1}{L}\sum_{l=1}^L\log p_\theta({\bf x}|{\bf z}^{(l)}),  　ただし{\bf z}^{(l)} = \mu + \sigma \odot \epsilon^{(l)}, \epsilon^{(l)}\sim N(0,{\bf I})
$$
そして、今回はデコーダーはベルヌーイ分布なため次の式に変換できる。
$$
  \frac{1}{L}\sum_{l=1}^L\log p_\theta({\bf x}|{\bf z}^{(l)})=\frac{1}{L}\sum_{l=1}^L \sum_{i=1}^D (x_i \log \lambda^{(l)}_i + (1-x_i)\log (1-\lambda^{(l)}_i)),　ただし\lambda^{(l)}=f_{\theta}({\bf z}^{(l)})
$$

第二項の正則化項は詳しくは割愛するが、次のようになる
$$
  D_{KL}[q_{\phi}({\bf z}|{\bf x})||p_{\theta}({\bf z})] = -\frac{1}{2}\sum_{j=1}^J(1+\log(\sigma_j^2)-\mu_j^2-\sigma_j^2), 　ただしp_{\theta}({\bf z})={\cal N}(0,{\bf I})
$$


